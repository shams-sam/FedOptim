{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import torch as t\n",
    "\n",
    "from fcn import FCN\n",
    "from utils import accumulate_grads_over_epochs , weights_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FCN(5, 1)\n",
    "for param in f.parameters():\n",
    "    print(param)\n",
    "\n",
    "weights_reset(f, [[1, 3], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in f.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.8628233671188354\n",
      "0.6535676717758179\n",
      "torch.Size([1, 7840]) torch.Size([1, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_sgd_batch_128_components_1.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.9662916837260127\n",
      "0.9481319785118103\n",
      "torch.Size([5, 7840]) torch.Size([5, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_sgd_batch_128_components_5.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.9999999229426066\n",
      "1.0000000605359705\n",
      "torch.Size([101, 7840]) torch.Size([10, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_sgd_batch_128_components_101.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.9558389186859131\n",
      "0.5722149014472961\n",
      "torch.Size([1, 7840]) torch.Size([1, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_sgd_batch_0_components_1.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.9999844329649932\n",
      "0.9999303127988242\n",
      "torch.Size([5, 7840]) torch.Size([5, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_sgd_batch_0_components_5.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.9999999858501262\n",
      "0.9999999289653099\n",
      "torch.Size([101, 7840]) torch.Size([10, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_sgd_batch_0_components_101.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.08749791234731674\n",
      "0.2848590910434723\n",
      "torch.Size([1, 7840]) torch.Size([1, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_adam_batch_128_components_1.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.2241940926760435\n",
      "0.8233962804079056\n",
      "torch.Size([5, 7840]) torch.Size([5, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_adam_batch_128_components_5.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "1.000000049782102\n",
      "1.0000000372529045\n",
      "torch.Size([101, 7840]) torch.Size([10, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_adam_batch_128_components_101.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.515287458896637\n",
      "0.6268476843833923\n",
      "torch.Size([1, 7840]) torch.Size([1, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_adam_batch_0_components_1.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.9616606067866087\n",
      "0.9911319259554148\n",
      "torch.Size([5, 7840]) torch.Size([5, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_adam_batch_0_components_5.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "1.0000000298468554\n",
      "1.000000045838538\n",
      "torch.Size([101, 7840]) torch.Size([10, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_adam_batch_0_components_101.pkl\n"
     ]
    }
   ],
   "source": [
    "paradigms = ['sgd', 'adam']\n",
    "batches = [128, 0]\n",
    "device = t.device(\"cuda:{}\".format(2))\n",
    "components = [1, 5, 101]\n",
    "\n",
    "for paradigm in paradigms:\n",
    "    for batch in batches:\n",
    "        for c in components:\n",
    "            file = '../ckpts/mnist_centralized/history/clf_fcn_noise_None' \\\n",
    "                '_paradigm_{}_lr_0.01_decay_1e-05_batch_{}.pkl'.format(paradigm, batch)\n",
    "            x_ax, acc_train, acc_test, l_train, l_test, grad = pkl.load(open(file, 'rb'))        \n",
    "            grad = accumulate_grads_over_epochs(grad, device)\n",
    "            grad0 = t.stack([_[0].flatten() for _ in grad], dim=0).T\n",
    "            grad1 = t.stack([_[1].flatten() for _ in grad], dim=0).T\n",
    "            print(grad0.size(), grad1.size())\n",
    "\n",
    "            if c==101:\n",
    "                pca = PCA()\n",
    "            else:\n",
    "                pca = PCA(n_components=c)\n",
    "            grad0 = pca.fit_transform(grad0.cpu().numpy())\n",
    "            print(sum(pca.explained_variance_ratio_[:c]))\n",
    "            grad1 = pca.fit_transform(grad1.cpu().numpy())\n",
    "            print(sum(pca.explained_variance_ratio_[:c]))\n",
    "            grad0 = t.Tensor(grad0).T.to(device)\n",
    "            grad1 = t.Tensor(grad1).T.to(device)\n",
    "            print(grad0.size(), grad1.size())\n",
    "            file = '../ckpts/mnist_centralized/data/grad'\\\n",
    "                '_pca_paradigm_{}_batch_{}_components_{}.pkl'.format(\n",
    "                paradigm, batch, c)\n",
    "            print('Saving: {}'.format(file))\n",
    "            pkl.dump((grad0, grad1), open(file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.9558516144752502\n",
      "0.5722872018814087\n",
      "torch.Size([1, 7840]) torch.Size([1, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_sgd_batch_128_components_1_distributed_sim.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.9999840004820726\n",
      "0.9999304835801013\n",
      "torch.Size([5, 7840]) torch.Size([5, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_sgd_batch_128_components_5_distributed_sim.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "1.0000001002948704\n",
      "0.9999999349220164\n",
      "torch.Size([101, 7840]) torch.Size([10, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_sgd_batch_128_components_101_distributed_sim.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.9558390974998474\n",
      "0.5722148418426514\n",
      "torch.Size([1, 7840]) torch.Size([1, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_sgd_batch_0_components_1_distributed_sim.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.9999844335616217\n",
      "0.9999303366057575\n",
      "torch.Size([5, 7840]) torch.Size([5, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_sgd_batch_0_components_5_distributed_sim.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.9999999957162027\n",
      "0.9999999530286774\n",
      "torch.Size([101, 7840]) torch.Size([10, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_sgd_batch_0_components_101_distributed_sim.pkl\n"
     ]
    }
   ],
   "source": [
    "paradigms = ['sgd']\n",
    "batches = [128, 0]\n",
    "device = t.device(\"cuda:{}\".format(2))\n",
    "components = [1, 5, 101]\n",
    "\n",
    "for paradigm in paradigms:\n",
    "    for batch in batches:\n",
    "        for c in components:\n",
    "            file = '../ckpts/mnist_centralized/history/clf_fcn_noise_None' \\\n",
    "                '_paradigm_{}_lr_0.01_decay_1e-05_batch_{}_distributed_sim.pkl'.format(paradigm, batch)\n",
    "            x_ax, acc_train, acc_test, l_train, l_test, grad = pkl.load(open(file, 'rb'))        \n",
    "            grad = accumulate_grads_over_epochs(grad, device)\n",
    "            grad0 = t.stack([_[0].flatten() for _ in grad], dim=0).T\n",
    "            grad1 = t.stack([_[1].flatten() for _ in grad], dim=0).T\n",
    "            print(grad0.size(), grad1.size())\n",
    "\n",
    "            if c==101:\n",
    "                pca = PCA()\n",
    "            else:\n",
    "                pca = PCA(n_components=c)\n",
    "            grad0 = pca.fit_transform(grad0.cpu().numpy())\n",
    "            print(sum(pca.explained_variance_ratio_[:c]))\n",
    "            grad1 = pca.fit_transform(grad1.cpu().numpy())\n",
    "            print(sum(pca.explained_variance_ratio_[:c]))\n",
    "            grad0 = t.Tensor(grad0).T.to(device)\n",
    "            grad1 = t.Tensor(grad1).T.to(device)\n",
    "            print(grad0.size(), grad1.size())\n",
    "            file = '../ckpts/mnist_centralized/data/grad'\\\n",
    "                '_pca_paradigm_{}_batch_{}_components_{}_distributed_sim.pkl'.format(\n",
    "                paradigm, batch, c)\n",
    "            print('Saving: {}'.format(file))\n",
    "            pkl.dump((grad0, grad1), open(file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.5153853297233582\n",
      "0.6270273327827454\n",
      "torch.Size([1, 7840]) torch.Size([1, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_adam_batch_128_components_1_distributed_sim.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.9616579543799162\n",
      "0.9911265401169658\n",
      "torch.Size([5, 7840]) torch.Size([5, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_adam_batch_128_components_5_distributed_sim.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.9999999662483817\n",
      "0.9999999520368886\n",
      "torch.Size([101, 7840]) torch.Size([10, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_adam_batch_128_components_101_distributed_sim.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.5152881145477295\n",
      "0.6268478035926819\n",
      "torch.Size([1, 7840]) torch.Size([1, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_adam_batch_0_components_1_distributed_sim.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "0.9616605639457703\n",
      "0.9911319157108665\n",
      "torch.Size([5, 7840]) torch.Size([5, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_adam_batch_0_components_5_distributed_sim.pkl\n",
      "torch.Size([7840, 101]) torch.Size([10, 101])\n",
      "1.0000000627079233\n",
      "1.0000000332365795\n",
      "torch.Size([101, 7840]) torch.Size([10, 10])\n",
      "Saving: ../ckpts/mnist_centralized/data/grad_pca_paradigm_adam_batch_0_components_101_distributed_sim.pkl\n"
     ]
    }
   ],
   "source": [
    "paradigms = ['adam']\n",
    "batches = [128, 0]\n",
    "device = t.device(\"cuda:{}\".format(1))\n",
    "components = [1, 5, 101]\n",
    "\n",
    "for paradigm in paradigms:\n",
    "    for batch in batches:\n",
    "        for c in components:\n",
    "            file = '../ckpts/mnist_centralized/history/clf_fcn_noise_None' \\\n",
    "                '_paradigm_{}_lr_0.01_decay_1e-05_batch_{}_distributed_sim.pkl'.format(paradigm, batch)\n",
    "            x_ax, acc_train, acc_test, l_train, l_test, grad = pkl.load(open(file, 'rb'))        \n",
    "            grad = accumulate_grads_over_epochs(grad, device)\n",
    "            grad0 = t.stack([_[0].flatten() for _ in grad], dim=0).T\n",
    "            grad1 = t.stack([_[1].flatten() for _ in grad], dim=0).T\n",
    "            print(grad0.size(), grad1.size())\n",
    "\n",
    "            if c==101:\n",
    "                pca = PCA()\n",
    "            else:\n",
    "                pca = PCA(n_components=c)\n",
    "            grad0 = pca.fit_transform(grad0.cpu().numpy())\n",
    "            print(sum(pca.explained_variance_ratio_[:c]))\n",
    "            grad1 = pca.fit_transform(grad1.cpu().numpy())\n",
    "            print(sum(pca.explained_variance_ratio_[:c]))\n",
    "            grad0 = t.Tensor(grad0).T.to(device)\n",
    "            grad1 = t.Tensor(grad1).T.to(device)\n",
    "            print(grad0.size(), grad1.size())\n",
    "            file = '../ckpts/mnist_centralized/data/grad'\\\n",
    "                '_pca_paradigm_{}_batch_{}_components_{}_distributed_sim.pkl'.format(\n",
    "                paradigm, batch, c)\n",
    "            print('Saving: {}'.format(file))\n",
    "            pkl.dump((grad0, grad1), open(file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t.Tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "b = t.Tensor([ 0.0144, -0.0036,  0.0136,  0.0283,  0.0253,  0.0023, -0.0070, -0.0400, 0.0216, -0.0116]) \n",
    "topk = t.argsort(b)\n",
    "a[topk] = a[topk] + b[topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "gi, gii, topk = pkl.load(open('../src/tmp.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "gi[topk] = gi[topk] + gii[topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_epoch, h_acc_train, h_acc_test, h_loss_train, h_loss_test, h_grads = pkl.load(open('../ckpts/mnist_centralized/history/clf_fcn_paradigm_sgd_lr_0.1_decay_1e-05_batch_256.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_grads = []\n",
    "for gradi in h_grads:\n",
    "    gradii = [torch.zeros(_.shape) for _ in gradi[0]]\n",
    "    for idx in range(len(gradi)):\n",
    "        gradii = [i + ii for i, ii in zip(gradi[idx], gradii)]\n",
    "    agg_grads.append([_/3 for _ in gradii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 784])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradii[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0287, -0.0287, -0.0287,  ..., -0.0287, -0.0287, -0.0287],\n",
       "         [-0.0651, -0.0651, -0.0651,  ..., -0.0651, -0.0651, -0.0651],\n",
       "         [ 0.0477,  0.0477,  0.0477,  ...,  0.0477,  0.0477,  0.0477],\n",
       "         ...,\n",
       "         [ 0.0341,  0.0341,  0.0341,  ...,  0.0341,  0.0341,  0.0341],\n",
       "         [-0.0558, -0.0558, -0.0558,  ..., -0.0558, -0.0558, -0.0558],\n",
       "         [-0.0475, -0.0475, -0.0475,  ..., -0.0475, -0.0475, -0.0475]]),\n",
       " tensor([ 0.0676,  0.1535, -0.1126, -0.1136,  0.1373, -0.3234,  0.0280, -0.0804,\n",
       "          0.1316,  0.1119])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad1 = [_.flatten() for _ in agg_grads[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.0096, -0.0096, -0.0096,  ..., -0.0158, -0.0158, -0.0158]),\n",
       " tensor([ 0.0225,  0.0512, -0.0375, -0.0379,  0.0458, -0.1078,  0.0093, -0.0268,\n",
       "          0.0439,  0.0373])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
